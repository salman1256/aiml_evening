{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mrZvNnbghhFI"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_3jzMBjKiePR",
        "outputId": "fc272b63-b386-433f-bddd-9ca237b18e78"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "our_text=\"Artificial Intelligence and Machine Learning are going to change the future of technology!\""
      ],
      "metadata": {
        "id": "K4OwSvU5j5Ic"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokens=word_tokenize(our_text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbeAwdCWkN3O",
        "outputId": "d68d18f2-4d40-4be5-dc95-507bde78f925"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Artificial', 'Intelligence', 'and', 'Machine', 'Learning', 'are', 'going', 'to', 'change', 'the', 'future', 'of', 'technology', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning\n",
        "clean_tokens=[re.sub('[^a-zA-Z0-9]','',token.lower()) for token in tokens if token.isalpha()]\n",
        "print(clean_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynGsVgROkYuT",
        "outputId": "6d675cb9-5b04-4c7e-ca03-9ae02cd569ae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'and', 'machine', 'learning', 'are', 'going', 'to', 'change', 'the', 'future', 'of', 'technology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stop Words Removal\n",
        "#1. Stop Word set of english\n",
        "stop_words=set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wbW1SGRnmbMS",
        "outputId": "c984b04b-a583-41db-eaca-ef15e1804e30"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'it', 'own', \"mightn't\", 't', 'ma', \"shouldn't\", \"you're\", \"we'd\", \"wasn't\", 'over', \"they'll\", 'there', \"we've\", 'to', 'whom', 'themselves', 'further', 'some', 've', 'mustn', \"you've\", 'my', 'myself', 'itself', 'most', \"she'd\", 'these', 'them', 'won', \"i'll\", 'hers', 'now', \"we'll\", 'between', 'each', 'her', \"hadn't\", \"she's\", \"shan't\", 'from', \"it's\", \"i'm\", \"we're\", 'wouldn', 'd', 'his', \"wouldn't\", 'below', \"isn't\", 'off', 'that', 'him', 'shouldn', 'on', 'weren', 'after', 'is', 'don', 'haven', 'himself', 'then', 'isn', 'should', 'was', 'yourselves', 'o', 'for', 'at', \"you'd\", 'wasn', 'again', 'ain', 'herself', 'are', 're', 'ourselves', \"they're\", \"aren't\", 'm', \"you'll\", \"that'll\", 'its', 'those', 'here', 'what', 'not', 'both', 'while', 'because', 'did', 'aren', 'had', 'hadn', 'too', \"he's\", 'with', 'a', 'he', 'will', 'being', 'can', 'didn', \"mustn't\", 'if', 'above', \"he'd\", 'against', 'nor', 'until', \"won't\", 'few', 'how', 'an', 'very', \"i've\", 'does', \"should've\", 'only', 'ours', 'y', 'shan', 'through', 'into', 'up', 'no', 'am', 'where', 'their', \"he'll\", 'or', 'in', 'yourself', \"weren't\", 'our', \"she'll\", 'than', 'under', 's', 'you', 'be', \"needn't\", 'down', \"hasn't\", 'having', 'when', \"they'd\", 'mightn', 'doesn', 'why', 'other', 'll', 'just', 'about', 'during', 'before', 'and', 'same', \"doesn't\", \"it'll\", 'by', 'which', 'who', 'this', 'all', 'once', \"it'd\", 'so', 'yours', 'of', \"i'd\", 'the', 'but', 'hasn', 'i', 'needn', 'me', 'such', \"haven't\", 'they', 'we', 'any', 'do', 'out', \"don't\", 'she', 'couldn', 'were', 'been', 'your', \"couldn't\", \"didn't\", 'theirs', 'as', 'doing', 'have', 'has', \"they've\", 'more'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens=[token for token in clean_tokens if token not in stop_words] #2. stop  word removal example\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwSPeh9nnPci",
        "outputId": "4b91594b-9ee5-41ff-a431-5d5704c0fd5e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'machine', 'learning', 'going', 'change', 'future', 'technology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming: Reduce the words to their root form (It's not always a dictionary word)\n",
        "stemmer=PorterStemmer()\n",
        "stemmed_tokens=[stemmer.stem(token) for token in filtered_tokens]\n",
        "print(stemmed_tokens)\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TOF0nKtoDd1",
        "outputId": "df52b537-8cab-4c4e-b6fa-56e3bae8ee08"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artifici', 'intellig', 'machin', 'learn', 'go', 'chang', 'futur', 'technolog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization: Convert words to their dictionary form (lemma) using POS: part of speech\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "lemmatized_tokens=[lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44wqT1z4oiJD",
        "outputId": "48302840-7833-4d89-919a-78cb9773ccba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'machine', 'learning', 'going', 'change', 'future', 'technology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Vectorization: Converting text tokens into numbers so ML models can understand them.\n",
        "#Vectorization expect sentences not token list\n",
        "text=[\" \".join(lemmatized_tokens)]\n",
        "print (text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOoL-hxmp-NW",
        "outputId": "88251d87-b6ac-43f6-d819-d21b984bf734"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial intelligence machine learning going change future technology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=CountVectorizer()\n",
        "X=vectorizer.fit_transform(text)\n",
        "print(vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQZSbCNzqldv",
        "outputId": "c8dc6c69-4b3f-47fd-c44b-7c36da920afc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial' 'change' 'future' 'going' 'intelligence' 'learning'\n",
            " 'machine' 'technology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHbn9_RCq9Le",
        "outputId": "98764366-8b13-41a9-ca40-327da0acc4a0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=[\n",
        "    'Artificial Intelligence is future.',\n",
        "    'Machine Learning is a subset of Artificial Intelligence',\n",
        "    'Deep Learning is a subset of Machine Learning and Machine Learing is importent for Artificial Intelligence',\n",
        "    'Natural Language Processing is also a part of Machine Learning and Artificial Intelligence',\n",
        "    'Computer Vision is also a part of Machine Learning and Artificial Intelligence'\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "ExdqQUpRyfST"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokens=[word_tokenize(sentence.lower()) for sentence in dataset]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FSev2tur0Q9F",
        "outputId": "82ca056c-abff-4fe8-dd05-70fea7212e1c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['artificial', 'intelligence', 'is', 'future', '.'], ['machine', 'learning', 'is', 'a', 'subset', 'of', 'artificial', 'intelligence'], ['deep', 'learning', 'is', 'a', 'subset', 'of', 'machine', 'learning', 'and', 'machine', 'learing', 'is', 'importent', 'for', 'artificial', 'intelligence'], ['natural', 'language', 'processing', 'is', 'also', 'a', 'part', 'of', 'machine', 'learning', 'and', 'artificial', 'intelligence'], ['computer', 'vision', 'is', 'also', 'a', 'part', 'of', 'machine', 'learning', 'and', 'artificial', 'intelligence']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stop Words Removal\n",
        "stop_words=set(stopwords.words('english'))\n",
        "filtered_tokens=[[token for token in sentence if token.isalpha() and token not in stop_words] for sentence in tokens]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qGTDPd31Y0i",
        "outputId": "3d714fae-d86e-44d9-b36c-c37beb46f9a8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['artificial', 'intelligence', 'future'], ['machine', 'learning', 'subset', 'artificial', 'intelligence'], ['deep', 'learning', 'subset', 'machine', 'learning', 'machine', 'learing', 'importent', 'artificial', 'intelligence'], ['natural', 'language', 'processing', 'also', 'part', 'machine', 'learning', 'artificial', 'intelligence'], ['computer', 'vision', 'also', 'part', 'machine', 'learning', 'artificial', 'intelligence']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "lemmatized_tokens=[[lemmatizer.lemmatize(token) for token in sentence] for sentence in filtered_tokens]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9Hv1JWaR2SAw",
        "outputId": "279433e5-849f-4ec5-a6ad-7b2b16233271"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['artificial', 'intelligence', 'future'], ['machine', 'learning', 'subset', 'artificial', 'intelligence'], ['deep', 'learning', 'subset', 'machine', 'learning', 'machine', 'learing', 'importent', 'artificial', 'intelligence'], ['natural', 'language', 'processing', 'also', 'part', 'machine', 'learning', 'artificial', 'intelligence'], ['computer', 'vision', 'also', 'part', 'machine', 'learning', 'artificial', 'intelligence']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_dataset=[\" \".join(sentence) for sentence in lemmatized_tokens]\n",
        "print(clean_dataset)\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JbOUbyNf2wll",
        "outputId": "d9a6dcea-754c-4f99-95e0-98314a0aac8b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial intelligence future', 'machine learning subset artificial intelligence', 'deep learning subset machine learning machine learing importent artificial intelligence', 'natural language processing also part machine learning artificial intelligence', 'computer vision also part machine learning artificial intelligence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow_vectorizer=CountVectorizer()\n",
        "X=bow_vectorizer.fit_transform(clean_dataset)\n"
      ],
      "metadata": {
        "id": "yZVwlA1O2-wt"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bow_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_lIk_fxj3ERs",
        "outputId": "c5f1ec17-360e-4f49-8c78-db0c99cc1452"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['also' 'artificial' 'computer' 'deep' 'future' 'importent' 'intelligence'\n",
            " 'language' 'learing' 'learning' 'machine' 'natural' 'part' 'processing'\n",
            " 'subset' 'vision']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Bag of Words Matrix')\n",
        "print(X.toarray())\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SXAQErWF3Ie6",
        "outputId": "232eee4a-bac5-4bb1-a9db-160020cee12d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Matrix\n",
            "[[0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0]\n",
            " [0 1 0 1 0 1 1 0 1 2 2 0 0 0 1 0]\n",
            " [1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0]\n",
            " [1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS1hnG-V5I0i",
        "outputId": "fc9d6a1f-9adf-4951-c41a-92b39af6475b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial intelligence machine learning going change future technology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=TfidfVectorizer()\n",
        "X=vectorizer.fit_transform(text)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4yphfI95VSZ",
        "outputId": "e3b449cd-4e03-4ca7-b420-06b74385d894"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial' 'change' 'future' 'going' 'intelligence' 'learning'\n",
            " 'machine' 'technology']\n",
            "[[0.35355339 0.35355339 0.35355339 0.35355339 0.35355339 0.35355339\n",
            "  0.35355339 0.35355339]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movie_review=[\n",
        "    'I love this movie',\n",
        "    'This movie is excellent',\n",
        "    'I hate this movie because too much fight',\n",
        "    'I am a fan of this product',\n",
        "    'I hate this product',\n",
        "    'It is truly a masterpiece ,great movie'\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "jxhqnWTH6Axl"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=TfidfVectorizer()\n",
        "X=vectorizer.fit_transform(movie_review)\n"
      ],
      "metadata": {
        "id": "e8chvfvk62BX"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI3OvOWG7A4d",
        "outputId": "a0a65dcf-b364-4e01-c070-a0898cd66442"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         am   because  excellent  ...      this       too     truly\n",
            "0  0.000000  0.000000   0.000000  ...  0.403215  0.000000  0.000000\n",
            "1  0.000000  0.000000   0.661272  ...  0.338787  0.000000  0.000000\n",
            "2  0.000000  0.434912   0.000000  ...  0.222817  0.434912  0.000000\n",
            "3  0.504119  0.000000   0.000000  ...  0.258274  0.000000  0.000000\n",
            "4  0.000000  0.000000   0.000000  ...  0.404106  0.000000  0.000000\n",
            "5  0.000000  0.000000   0.000000  ...  0.000000  0.000000  0.446127\n",
            "\n",
            "[6 rows x 18 columns]\n"
          ]
        }
      ]
    }
  ]
}